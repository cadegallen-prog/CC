{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Product Type Classification Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook establishes the ML pipeline structure for product type classification of Home Depot catalog items. It is designed as a **scaffold awaiting organic labels** derived from the dataset itself.\n",
    "\n",
    "## Architecture\n",
    "- **Data Source**: `data/scraped_data_output.json` (425 product records)\n",
    "- **Features**: TF-IDF vectorization of combined `title` + `description` fields\n",
    "- **Model**: Logistic Regression classifier (scikit-learn)\n",
    "- **Split Strategy**: 80/20 train/test with stratification on unique title/model combinations\n",
    "\n",
    "## Label Integration Strategy\n",
    "Once organic product-type labels are established through data-driven discovery:\n",
    "1. **Insert labels** in the designated section below (see `TODO: INSERT ORGANIC LABELS`)\n",
    "2. Labels should be derived from title/description patterns, brand signals, and attribute clusters\n",
    "3. Each product record needs a `product_type` field added to the DataFrame\n",
    "4. Do NOT force-fit items into taxonomy leaves until confident labels exist\n",
    "\n",
    "## Feature Engineering Notes\n",
    "- **Current**: Simple TF-IDF on text concatenation\n",
    "- **Future enhancements**:\n",
    "  - Brand embeddings or one-hot encoding\n",
    "  - Extracted attributes (dimensions, wattage, material) from `structured_specifications`\n",
    "  - N-gram keyword buckets\n",
    "  - Sentence embeddings (SentenceTransformers)\n",
    "\n",
    "## Important Notes\n",
    "- This notebook will run in skeleton form without errors\n",
    "- No taxonomy file references are included (taxonomy mapping comes later)\n",
    "- All TODOs must be addressed before actual training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scraped product data\n",
    "data_path = Path('../data/scraped_data_output.json')\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    products_raw = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(products_raw)\n",
    "\n",
    "print(f\"Loaded {len(df)} product records\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nSample record:\")\n",
    "display(df[['title', 'brand', 'model', 'price']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration & Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in key fields\n",
    "print(\"Missing values:\")\n",
    "print(df[['title', 'description', 'brand', 'model']].isnull().sum())\n",
    "\n",
    "# Unique titles and models (for split preservation)\n",
    "print(f\"\\nUnique titles: {df['title'].nunique()}\")\n",
    "print(f\"Unique models: {df['model'].nunique()}\")\n",
    "print(f\"Unique brands: {df['brand'].nunique()}\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined text field for TF-IDF\n",
    "# Handle missing descriptions gracefully\n",
    "df['description'] = df['description'].fillna('')\n",
    "df['title'] = df['title'].fillna('')\n",
    "\n",
    "# Combine title and description\n",
    "df['combined_text'] = df['title'] + ' ' + df['description']\n",
    "\n",
    "print(f\"‚úì Created combined_text field\")\n",
    "print(f\"Average text length: {df['combined_text'].str.len().mean():.0f} characters\")\n",
    "\n",
    "# Preview combined text\n",
    "print(\"\\nSample combined text (first 200 chars):\")\n",
    "print(df['combined_text'].iloc[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ‚ö†Ô∏è ORGANIC LABEL INTEGRATION POINT ‚ö†Ô∏è\n",
    "\n",
    "### TODO: INSERT ORGANIC LABELS HERE WHEN READY\n",
    "\n",
    "Before training can begin, you must:\n",
    "\n",
    "1. **Discover organic product types** from the dataset using:\n",
    "   - Title/description keyword clustering\n",
    "   - Brand-category associations\n",
    "   - Attribute patterns from `structured_specifications`\n",
    "   - Manual review of high-confidence clusters\n",
    "\n",
    "2. **Add a `product_type` column** to the DataFrame:\n",
    "   ```python\n",
    "   # Example (replace with actual labels):\n",
    "   df['product_type'] = None  # Fill with discovered labels\n",
    "   ```\n",
    "\n",
    "3. **Validation requirements**:\n",
    "   - At least 5-10 examples per product type\n",
    "   - Clear distinction between types\n",
    "   - Confidence scores for each assignment\n",
    "\n",
    "4. **Do NOT**:\n",
    "   - Force-fit items into taxonomy_paths.txt categories yet\n",
    "   - Use arbitrary synthetic labels\n",
    "   - Skip human review of uncertain assignments (<0.7 confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PLACEHOLDER: Organic labels will be inserted here\n",
    "# ============================================================================\n",
    "\n",
    "# For now, create a dummy placeholder to allow the pipeline to run\n",
    "# This will be replaced with actual organic labels from data discovery\n",
    "df['product_type'] = 'UNLABELED'  # Placeholder - replace with real labels\n",
    "\n",
    "print(\"‚ö†Ô∏è  WARNING: Using placeholder labels\")\n",
    "print(\"‚ö†Ô∏è  Training cannot proceed until organic labels are added\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['product_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split\n",
    "\n",
    "**Strategy**: 80/20 split preserving unique titles/models to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create unique identifier for deduplication\ndf['unique_id'] = df['title'] + '_' + df['model'].astype(str)\n\n# Check for duplicates\nn_duplicates = df['unique_id'].duplicated().sum()\nif n_duplicates > 0:\n    print(f\"‚ö†Ô∏è  Found {n_duplicates} duplicate records (same title+model)\")\n    print(f\"Removing duplicates to prevent data leakage...\")\n    df = df.drop_duplicates(subset='unique_id', keep='first')\n    print(f\"‚úì Deduplicated: {len(df)} unique records remaining\")\nelse:\n    print(f\"‚úì No duplicates found\")\n\n# Perform stratified split based on product_type\n# Note: This will fail with single class, but structure is ready for multi-class\ntry:\n    train_df, test_df = train_test_split(\n        df,\n        test_size=0.2,\n        random_state=42,\n        stratify=df['product_type']  # Stratify when labels are available\n    )\n    print(f\"‚úì Stratified split successful\")\nexcept ValueError as e:\n    # Fallback to simple random split if stratification fails (e.g., single class)\n    print(f\"‚ÑπÔ∏è  Stratification not possible (expected with placeholder labels)\")\n    train_df, test_df = train_test_split(\n        df,\n        test_size=0.2,\n        random_state=42\n    )\n    print(f\"‚úì Simple random split performed\")\n\nprint(f\"\\nTrain set: {len(train_df)} records ({len(train_df)/len(df)*100:.1f}%)\")\nprint(f\"Test set: {len(test_df)} records ({len(test_df)/len(df)*100:.1f}%)\")\n\n# Verify no title/model overlap between train and test\ntrain_ids = set(train_df['unique_id'])\ntest_ids = set(test_df['unique_id'])\noverlap = train_ids & test_ids\nassert len(overlap) == 0, f\"Data leakage detected: {len(overlap)} overlapping IDs\"\nprint(f\"‚úì Data leakage check passed: 0 overlapping IDs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TF-IDF Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "# Parameters tuned for product descriptions\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,      # Limit vocabulary size\n",
    "    min_df=2,               # Ignore terms appearing in fewer than 2 documents\n",
    "    max_df=0.8,             # Ignore terms appearing in >80% of documents\n",
    "    ngram_range=(1, 2),     # Unigrams and bigrams\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit on training data only (prevent test set leakage)\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['combined_text'])\n",
    "X_test_tfidf = tfidf.transform(test_df['combined_text'])\n",
    "\n",
    "print(f\"‚úì TF-IDF vectorization complete\")\n",
    "print(f\"Feature matrix shape (train): {X_train_tfidf.shape}\")\n",
    "print(f\"Feature matrix shape (test): {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Show top features by IDF score\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "idf_scores = tfidf.idf_\n",
    "top_features_idx = np.argsort(idf_scores)[:20]  # Lowest IDF = most common\n",
    "print(f\"\\nSample features (most common): {[feature_names[i] for i in top_features_idx[:10]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Classifier Scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression classifier\n",
    "# Using parameters suitable for multi-class text classification\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,           # Sufficient iterations for convergence\n",
    "    multi_class='multinomial',  # Proper multi-class handling\n",
    "    solver='lbfgs',          # Efficient for small-medium datasets\n",
    "    random_state=42,\n",
    "    class_weight='balanced', # Handle class imbalance\n",
    "    n_jobs=-1                # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(f\"‚úì Classifier initialized: {clf.__class__.__name__}\")\n",
    "print(f\"Parameters: {clf.get_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ‚ö†Ô∏è TRAINING LOOP PLACEHOLDER ‚ö†Ô∏è\n",
    "\n",
    "### TODO: ADD TRAINING LOOP AFTER LABELS ARE READY\n",
    "\n",
    "Once organic labels are integrated, uncomment and run the training code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TODO: UNCOMMENT THIS SECTION AFTER ADDING ORGANIC LABELS\n",
    "# ============================================================================\n",
    "\n",
    "# # Extract labels\n",
    "# y_train = train_df['product_type']\n",
    "# y_test = test_df['product_type']\n",
    "\n",
    "# # Train the classifier\n",
    "# print(\"Training classifier...\")\n",
    "# clf.fit(X_train_tfidf, y_train)\n",
    "# print(\"‚úì Training complete\")\n",
    "\n",
    "# # Make predictions\n",
    "# y_train_pred = clf.predict(X_train_tfidf)\n",
    "# y_test_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# # Evaluate\n",
    "# train_acc = accuracy_score(y_train, y_train_pred)\n",
    "# test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"\\nTraining accuracy: {train_acc:.4f}\")\n",
    "# print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "# print(f\"\\nClassification Report (Test Set):\")\n",
    "# print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# # Check if meets >=98% accuracy requirement\n",
    "# if test_acc >= 0.98:\n",
    "#     print(\"‚úì Model meets >=98% accuracy threshold\")\n",
    "# else:\n",
    "#     print(f\"‚ö†Ô∏è  Model accuracy ({test_acc:.2%}) below 98% threshold\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  Training code is ready but waiting for organic labels\")\n",
    "print(\"‚ö†Ô∏è  Uncomment the section above after label integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pipeline Status Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASELINE PRODUCT TYPE CLASSIFICATION PIPELINE - STATUS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úì Data loaded: {len(df)} records\")\n",
    "print(f\"‚úì Features engineered: TF-IDF with {X_train_tfidf.shape[1]} features\")\n",
    "print(f\"‚úì Train/test split: {len(train_df)}/{len(test_df)} (80/20)\")\n",
    "print(f\"‚úì Classifier scaffold ready: {clf.__class__.__name__}\")\n",
    "print(f\"\\n‚ö†Ô∏è  BLOCKERS:\")\n",
    "print(f\"   1. Organic product-type labels not yet assigned\")\n",
    "print(f\"   2. Training loop commented out pending labels\")\n",
    "print(f\"\\nüìã NEXT STEPS:\")\n",
    "print(f\"   1. Run data discovery to identify organic product types\")\n",
    "print(f\"   2. Assign product_type labels in Section 5\")\n",
    "print(f\"   3. Uncomment training code in Section 9\")\n",
    "print(f\"   4. Validate >=98% accuracy requirement\")\n",
    "print(f\"   5. Map confident labels to taxonomy (future phase)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes\n",
    "\n",
    "### Model Persistence (Future)\n",
    "Once trained, save the model and vectorizer:\n",
    "```python\n",
    "import joblib\n",
    "joblib.dump(clf, '../models/product_type_classifier.pkl')\n",
    "joblib.dump(tfidf, '../models/tfidf_vectorizer.pkl')\n",
    "```\n",
    "\n",
    "### Confidence Scoring (Future)\n",
    "Use `predict_proba()` to identify uncertain predictions:\n",
    "```python\n",
    "proba = clf.predict_proba(X_test_tfidf)\n",
    "max_proba = proba.max(axis=1)\n",
    "uncertain = max_proba < 0.7  # Flag for human review\n",
    "```\n",
    "\n",
    "### Feature Enhancement Ideas\n",
    "1. Add brand as categorical feature\n",
    "2. Extract numeric attributes (wattage, dimensions) from structured_specifications\n",
    "3. Use SentenceTransformers embeddings instead of TF-IDF\n",
    "4. Include price bins as features\n",
    "5. Leverage rating/review count as quality signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}